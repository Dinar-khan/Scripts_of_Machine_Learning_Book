{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Example - Neural Network\n",
    "### Backend Computation of Feed-Forward Network & Backpropagation\n",
    "\n",
    "we will take a look at a very simple artificial neural network. \n",
    "We will create a neural network that will imitate a special OR function with two inputs and one output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signoimd Function\n",
    "\n",
    "This is a sigmoid function which is non-linear by type. We have selected it for this neural network. It has simple analytical features and is easy to use and understand. This function does two things: 1). compute sigmoid 2.) calculate derivative\n",
    "\n",
    "By default, *deriv = False*, it compute sigmoid. If set to *deriv = True*, it calculates the derivative of the function used in the back propagation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function \n",
    "def nonlin(x, deriv=False):\n",
    "  if(deriv==True):\n",
    "     return (x*(1-x))\n",
    "  return 1/(1+np.exp(-x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input matrix\n",
    "We will create an input matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input feature set\n",
    "X = np.array([[0,0,1], \n",
    "      [0,1,1], \n",
    "      [1,0,1],\n",
    "      [1,1,1]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable\n",
    "y = np.array([[0], \n",
    "        [1], \n",
    "        [1], \n",
    "        [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed to random generator so that it returns the same random numbers each time for reproducibility.\n",
    "np.random.seed(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons / Weightages\n",
    "\n",
    "Now we need to create neurons. We will then start the weight with random values. \n",
    "\n",
    "syn0 is the weight between the input layer and the hidden layer. It is a 3x4 order matrix because the hidden layer contains two input weights plus the value of bias term (= 3) and four nodes (= 4). \n",
    "\n",
    "syn1 is the weight between the hidden layer and the output layer. This is a 4x1 order matrix because the hidden layer contains 4 nodes and one output. \n",
    "\n",
    "In this example there is no bias term for the feeding of output layer. Initially the weights are randomly generated because optimization does not work well when all the weights start at the same value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Train Model\n",
    "\n",
    "This is the main loop of training. Output shows the evolution of error between the model and the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training step\n",
    "for j in range(60000): \n",
    "    # Feed-forward Network\n",
    "    # define layers as l0, l1, l2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0, syn0)) # perform dot product and apply activation function on hidden layer (3x4)\n",
    "    l2 = nonlin(np.dot(l1, syn1))  # perform dot product and apply activation function on output layer (4x1)\n",
    "    \n",
    "    l2_error = y - l2 # compute error from actual(y) to predicted (l2) - difference in desired values at l2\n",
    "    if(j % 10000) == 0: \n",
    "        print(\"Error: \" + str(np.mean(np.abs(l2_error)))) # MAE  \n",
    "        \n",
    "    # Backpropagation\n",
    "    l2_delta = l2_error*nonlin(l2, deriv=True) # to calibrate l2 weightages\n",
    "    l1_error = l2_delta.dot(syn1.T) # compute error at l1\n",
    "    l1_delta = l1_error * nonlin(l1, deriv=True) # to calibrate l1 weightages\n",
    "    \n",
    "    syn1 += l1.T.dot(l2_delta) # adjust syn1 weightages\n",
    "    syn0 += l0.T.dot(l1_delta) # adjust syn0 weightages\n",
    "    # after each iteration syn1 and syn0 will be updated\n",
    "\n",
    "\n",
    "print(\"This is the output when the training is finished\")\n",
    "print(l2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In the above example, observe how the neural network learns and makes mistakes.\n",
    "\n",
    "You can see how close the final output is to the actual output [0, 1, 1, 0]. \n",
    "If you increase the number of repetitions in the current training loop (currently 60,000), the final output will be more closer because there could be more repetitions for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
